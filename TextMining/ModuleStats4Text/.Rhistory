install.packages("tm") # Text mining
library(tm)
?DirSource
T <- DirSource("/home/mv/nltk_data/corpora/movie_reviews/neg", encoding = "UTF-8")
T
?Corpus
reviewsNeg <- Corpus(DirSource("/home/mv/nltk_data/corpora/movie_reviews/neg", encoding = "UTF-8"))
reviewsPos <- Corpus(DirSource("/home/mv/nltk_data/corpora/movie_reviews/pos", encoding = "UTF-8"))
reviewsNeg
class(reviewsNeg)
type
typeof
reviewsNeg[[1]]
reviewsNeg[[2]]
inspect(reviewsNeg[1:2])
reviewsNeg[[1]]
reviewsNeg[[1]]
reviews <- c(reviewsNeg,reviewsPos)
reviews
dtmReviews <- DocumentTermMatrix(reviews, control = list(removePunctuation = TRUE,
stopwords = TRUE,
minDocFreq=5,
removeNumbers=TRUE ))
dtmReviews
dtmReviews[1,]
dtmReviews[1,1]
inspect(dtmReviews[1:10,1000:1100])
inspect(dtmReviews[1:10,10:20])
dtmReviews[1:10,10:100]
which(dtmReviews[1:10,10:100]>0)
dtmReviews[1:10,10:100]>0
freqWords <- findFreqTerms(dtmReviews, 1000)
freqWords
dictFreqWords <- Dictionary(freqWords)
freqWords <- findFreqTerms(dtmReviews, 1000)
#dtmReviewsSparse <- removeSparseTerms(dtmReviews, 0.4)
dictFreqWords <- Dictionary(freqWords)
freqWords
dtmReduced <- DocumentTermMatrix(reviews, list(dictionary = freqWords))
dtmReduced
dtmReviews
dtmReduced
inspect(dtmReduced[1:10,])
y = c(rep('Neg',1000),rep('Pos',1000)) # Creates the response vector (i.e. the labels)
dtmDataFrame <- as.data.frame(inspect(dtmReduced))
y = c(rep('Neg',1000),rep('Pos',1000)) # Creates the response vector (i.e. the labels)
randomIdx <- sample(2000)
y <- y[randomIdx]
X <- dtmDataFrame[randomIdx,]
yTrain <- y[1:1500]
XTrain <- X[1:1500,]
yTest <- y[1501:2000]
XTest <- X[1501:2000,]
yTrain <- y[1:1500]
XTrain <- X[1:1500,]
yTest <- y[1501:2000]
XTest <- X[1501:2000,]
install.packages("e1071")
library(e1071)
?svm
model <- svm(yTrain ~ XTrain)
names(XTrain)
XTrain$action
names(XTrain)
model <- svm(yTrain ~ action + also,data=XTrain)
model <- svm(yTrain ~ XTrain$action + XTrain$also)
yTrain
model <- svm(XTrain$action,yTrain)
data(iris)
attach(iris)
model <- svm(Species ~ ., data = iris)
Species
yTrain
yTrain <- as.factor(y[1:1500])
XTrain <- X[1:1500,]
yTest <- as.factor(y[1501:2000])
XTest <- X[1501:2000,]
yTrain
model <- svm(XTrain$action,yTrain)
model <- svm(XTrain,yTrain)
Xtrain$doesnt
XTrain$doesnt
sum(XTrain$doesnt)
sum(XTrain)
rowsum(XTrain)
colsum
rowsim
rowsum
?rowsum
rowSums(XTrain)
rowSums(XTrain)
rowSums(XTrain)>0
appearAtLeastOnce <- rowSums(XTrain)>0
XTrain <- XTrain[,appearAtLeastOnce]
XTest <- XTest[,appearAtLeastOnce]
appearAtLeastOnce
XTrain <- XTrain[,appearAtLeastOnce]
appearAtLeastOnce <- colSums(XTrain)>0
appearAtLeastOnce
XTrain <- XTrain[,appearAtLeastOnce]
XTest <- XTest[,appearAtLeastOnce]
model <- svm(XTrain,yTrain)
model
svmModel <- svm(XTrain,yTrain)
print(svmModel)
summary(svmModel)
summary(svmModel)
print(svmModel)
pred <- predict(svmModel,XTest)
pred
table(pred, yTest)
?predict
confusionMatrix <- table(pred, yTest)
confusionMatrix
confusionMatrix[1,1]/sum(confusionMatrix[1,])
confusionMatrix[1,1]/sum(confusionMatrix[,1])
nb
?naiveBayes
naiveBayes(yTrain ~ XTrain)
naiveBayes(yTrain ~ XTrain, data = XTrain)
reviewsData <- as.data.frame(yTrain,XTrain)
reviewsData <- as.data.frame(cbind(yTrain,XTrain))
names(reviewsData)
names(reviewsData[[1]])
names(reviewsData[[2]])
names(reviewsData[1])
names(reviewsData[1]) <- 'y'
names(reviewsData)
names(reviewsData[1]) <- 'y'
names(reviewsData)
names(reviewsData[2])
naiveBayes(yTrain ~ XTrain, data = reviewsData)
library("tm")
# Reading text from articles in Journal of Statistical Software using a special reader function.
install.packages("corpus.JSS.papers", repos = "http://datacube.wu.ac.at/", type = "source")
data("JSS_papers", package = "corpus.JSS.papers")
?dat
?data
JSS_papers
head(JSS_papers)
JSS_papers[,"date"]
dim(JSS_papers)
summary(JSS_papers)
JSS_papers <- JSS_papers[JSS_papers[,"date"] < "2010-08-05",]
Encoding
?Encoding
sapply(JSS_papers[, "description"], Encoding)
JSS_papers <- JSS_papers[sapply(JSS_papers[, "description"], Encoding) == "unknown",]
JSS_papers
library("XML")
install.packages("XML")
library("XML")
remove_HTML_markup <- function(s) tryCatch({ doc <- htmlTreeParse(paste("<!DOCTYPE html>", s), asText = TRUE, trim = FALSE)
xmlValue(xmlRoot(doc))}, error = function(s) s)
corpus <- Corpus(VectorSource(sapply(JSS_papers[, "description"], remove_HTML_markup)))
?VectorSource
corpus
install.packages("Snowball")
Sys.setlocale("LC_COLLATE", "C") # Language setting for the linguistic analysis
JSS_dtm <- DocumentTermMatrix(corpus, control = list(stemming = TRUE, stopwords = TRUE,
minWordLength = 3, removeNumbers = TRUE, removePunctuation = TRUE))
install.packages("Snowball")
JSS_dtm <- DocumentTermMatrix(corpus)
dim(JSS_dtm)
inspect(JSS_dtm[1:20,'algorithm'])
install.packages("slam")
install.packages("slam")
